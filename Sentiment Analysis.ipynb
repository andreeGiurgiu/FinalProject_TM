{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL \n",
    "\n",
    "Jan Korczynski -> jki750/2730097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import os \n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BOOKS_PATH = os.path.join('Books_rating.csv')\n",
    "SPORT_PATH = os.path.join('EAsports_FC24_Steam_Reviews.csv')\n",
    "MOVIE_PATH = os.path.join('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPORT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>review</th>\n",
       "      <th>voted_up</th>\n",
       "      <th>votes_up</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>steam_purchase</th>\n",
       "      <th>recieved_for_free</th>\n",
       "      <th>written_during_early_access</th>\n",
       "      <th>author_num_games_owned</th>\n",
       "      <th>author_num_reviews</th>\n",
       "      <th>author_playtime_forever</th>\n",
       "      <th>author_playtime_last_two_weeks</th>\n",
       "      <th>author_playtime_at_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157328534</td>\n",
       "      <td>english</td>\n",
       "      <td>absolute garbage</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>617</td>\n",
       "      <td>30</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157326188</td>\n",
       "      <td>english</td>\n",
       "      <td>9 out of 10 games of pvp will matchmake you ag...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>266</td>\n",
       "      <td>19</td>\n",
       "      <td>2960</td>\n",
       "      <td>2960</td>\n",
       "      <td>2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157325082</td>\n",
       "      <td>english</td>\n",
       "      <td>terrible game\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>8533</td>\n",
       "      <td>2790</td>\n",
       "      <td>8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157324209</td>\n",
       "      <td>english</td>\n",
       "      <td>h</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1199</td>\n",
       "      <td>1199</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157324180</td>\n",
       "      <td>english</td>\n",
       "      <td>I personally haven't experienced any major bug...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>965</td>\n",
       "      <td>965</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id language                                             review  \\\n",
       "0  157328534  english                                   absolute garbage   \n",
       "1  157326188  english  9 out of 10 games of pvp will matchmake you ag...   \n",
       "2  157325082  english                                    terrible game\\n   \n",
       "3  157324209  english                                                  h   \n",
       "4  157324180  english  I personally haven't experienced any major bug...   \n",
       "\n",
       "   voted_up  votes_up  comment_count  steam_purchase  recieved_for_free  \\\n",
       "0     False         0              0            True              False   \n",
       "1     False         1              0            True              False   \n",
       "2     False         0              0            True              False   \n",
       "3      True         0              0            True              False   \n",
       "4      True         0              0            True              False   \n",
       "\n",
       "   written_during_early_access  author_num_games_owned  author_num_reviews  \\\n",
       "0                        False                       0                   1   \n",
       "1                        False                     266                  19   \n",
       "2                        False                      84                   3   \n",
       "3                        False                       0                   1   \n",
       "4                        False                       0                   1   \n",
       "\n",
       "   author_playtime_forever  author_playtime_last_two_weeks  \\\n",
       "0                      617                              30   \n",
       "1                     2960                            2960   \n",
       "2                     8533                            2790   \n",
       "3                     1199                            1199   \n",
       "4                      965                             965   \n",
       "\n",
       "   author_playtime_at_review  \n",
       "0                        587  \n",
       "1                       2857  \n",
       "2                       8451  \n",
       "3                       1199  \n",
       "4                        965  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sport = pd.read_csv(SPORT_PATH)\n",
    "sport_data = df_sport.drop(['author_last_played', 'created'], axis=1)\n",
    "sport_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values distribution\n",
      "id                                0.000000\n",
      "language                          0.000000\n",
      "review                            0.003881\n",
      "voted_up                          0.000000\n",
      "votes_up                          0.000000\n",
      "comment_count                     0.000000\n",
      "steam_purchase                    0.000000\n",
      "recieved_for_free                 0.000000\n",
      "written_during_early_access       0.000000\n",
      "author_num_games_owned            0.000000\n",
      "author_num_reviews                0.000000\n",
      "author_playtime_forever           0.000000\n",
      "author_playtime_last_two_weeks    0.000000\n",
      "author_playtime_at_review         0.000000\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values distribution\")\n",
    "print(sport_data.isnull().mean())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column datatypes: \n",
      "id                                 int64\n",
      "language                          object\n",
      "review                            object\n",
      "voted_up                            bool\n",
      "votes_up                           int64\n",
      "comment_count                      int64\n",
      "steam_purchase                      bool\n",
      "recieved_for_free                   bool\n",
      "written_during_early_access         bool\n",
      "author_num_games_owned             int64\n",
      "author_num_reviews                 int64\n",
      "author_playtime_forever            int64\n",
      "author_playtime_last_two_weeks     int64\n",
      "author_playtime_at_review          int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Column datatypes: \")\n",
    "print(sport_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOVIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie = pd.read_csv(MOVIE_PATH)\n",
    "df_movie.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production.   The filming t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production.   The filming t...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(text):\n",
    "    return re.sub(r\"<br\\s*/?>\", \" \", text)\n",
    "\n",
    "\n",
    "df_movie['review'] = df_movie['review'].apply(clean_data)\n",
    "df_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column datatypes: \n",
      "review       object\n",
      "sentiment    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Column datatypes: \")\n",
    "print(df_movie.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOOKS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books = pd.read_csv(BOOKS_PATH)\n",
    "df_books.head()\n",
    "initial_length = len(df_books)\n",
    "initial_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values distribution\n",
      "Id                    0.000000\n",
      "Title                 0.000069\n",
      "Price                 0.839610\n",
      "User_id               0.187262\n",
      "profileName           0.187295\n",
      "review/helpfulness    0.000000\n",
      "review/score          0.000000\n",
      "review/time           0.000000\n",
      "review/summary        0.000013\n",
      "review/text           0.000003\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values distribution\")\n",
    "print(df_books.isnull().mean())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>User_id</th>\n",
       "      <th>profileName</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0829814000</td>\n",
       "      <td>Wonderful Worship in Smaller Churches</td>\n",
       "      <td>19.40</td>\n",
       "      <td>AZ0IOBU20TBOP</td>\n",
       "      <td>Rev. Pamela Tinnin</td>\n",
       "      <td>8/10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>991440000</td>\n",
       "      <td>Outstanding Resource for Small Church Pastors</td>\n",
       "      <td>I just finished the book, &amp;quot;Wonderful Wors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0829814000</td>\n",
       "      <td>Wonderful Worship in Smaller Churches</td>\n",
       "      <td>19.40</td>\n",
       "      <td>A373VVEU6Z9M0N</td>\n",
       "      <td>Dr. Terry W. Dorsett</td>\n",
       "      <td>1/1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1291766400</td>\n",
       "      <td>Small Churches CAN Have Wonderful Worship</td>\n",
       "      <td>Many small churches feel like they can not hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0829814000</td>\n",
       "      <td>Wonderful Worship in Smaller Churches</td>\n",
       "      <td>19.40</td>\n",
       "      <td>AGKGOH65VTRR4</td>\n",
       "      <td>Cynthia L. Lajoy \"Cindy La Joy\"</td>\n",
       "      <td>1/1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1248307200</td>\n",
       "      <td>Not Just for Pastors!</td>\n",
       "      <td>I just finished reading this amazing book and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0829814000</td>\n",
       "      <td>Wonderful Worship in Smaller Churches</td>\n",
       "      <td>19.40</td>\n",
       "      <td>A3OQWLU31BU1Y</td>\n",
       "      <td>Maxwell Grant</td>\n",
       "      <td>1/1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1222560000</td>\n",
       "      <td>Small church pastor? This is the book on worship</td>\n",
       "      <td>I hadn't been a small church pastor very long ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0595344550</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>10.95</td>\n",
       "      <td>A3Q12RK71N74LB</td>\n",
       "      <td>Book Reader</td>\n",
       "      <td>7/11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1117065600</td>\n",
       "      <td>not good</td>\n",
       "      <td>I bought this book because I read some glowing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                                  Title  Price         User_id  \\\n",
       "10  0829814000  Wonderful Worship in Smaller Churches  19.40   AZ0IOBU20TBOP   \n",
       "11  0829814000  Wonderful Worship in Smaller Churches  19.40  A373VVEU6Z9M0N   \n",
       "12  0829814000  Wonderful Worship in Smaller Churches  19.40   AGKGOH65VTRR4   \n",
       "13  0829814000  Wonderful Worship in Smaller Churches  19.40   A3OQWLU31BU1Y   \n",
       "14  0595344550          Whispers of the Wicked Saints  10.95  A3Q12RK71N74LB   \n",
       "\n",
       "                        profileName review/helpfulness  review/score  \\\n",
       "10               Rev. Pamela Tinnin               8/10           5.0   \n",
       "11             Dr. Terry W. Dorsett                1/1           5.0   \n",
       "12  Cynthia L. Lajoy \"Cindy La Joy\"                1/1           5.0   \n",
       "13                    Maxwell Grant                1/1           5.0   \n",
       "14                      Book Reader               7/11           1.0   \n",
       "\n",
       "    review/time                                    review/summary  \\\n",
       "10    991440000     Outstanding Resource for Small Church Pastors   \n",
       "11   1291766400         Small Churches CAN Have Wonderful Worship   \n",
       "12   1248307200                             Not Just for Pastors!   \n",
       "13   1222560000  Small church pastor? This is the book on worship   \n",
       "14   1117065600                                          not good   \n",
       "\n",
       "                                          review/text  \n",
       "10  I just finished the book, &quot;Wonderful Wors...  \n",
       "11  Many small churches feel like they can not hav...  \n",
       "12  I just finished reading this amazing book and ...  \n",
       "13  I hadn't been a small church pastor very long ...  \n",
       "14  I bought this book because I read some glowing...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.dropna(subset=['Price', 'User_id', 'profileName', 'review/summary', 'review/text', 'Title'], inplace=True)\n",
    "df_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows deleted 2585452\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows deleted {initial_length - len(df_books)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values distribution\n",
      "Id                    0.0\n",
      "Title                 0.0\n",
      "Price                 0.0\n",
      "User_id               0.0\n",
      "profileName           0.0\n",
      "review/helpfulness    0.0\n",
      "review/score          0.0\n",
      "review/time           0.0\n",
      "review/summary        0.0\n",
      "review/text           0.0\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values distribution\")\n",
    "print(df_books.isnull().mean())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentence id                                               text sentiment  \\\n",
      "0            0                                   absolute garbage   unknown   \n",
      "1            1  9 out of 10 games of pvp will matchmake you ag...   unknown   \n",
      "2            2                                    terrible game\\n   unknown   \n",
      "3            3                                                  h   unknown   \n",
      "4            4  I personally haven't experienced any major bug...   unknown   \n",
      "\n",
      "    topic  \n",
      "0  sports  \n",
      "1  sports  \n",
      "2  sports  \n",
      "3  sports  \n",
      "4  sports  \n"
     ]
    }
   ],
   "source": [
    "# Create a unified dataframe\n",
    "data = []\n",
    "\n",
    "# Add sports reviews\n",
    "for index, row in df_sport.iterrows():\n",
    "    data.append({'text': row['review'], 'sentiment': 'unknown', 'topic': 'sports'})\n",
    "\n",
    "# Add movie reviews\n",
    "for index, row in df_movie.iterrows():\n",
    "    data.append({'text': row['review'], 'sentiment': row['sentiment'], 'topic': 'movie'})\n",
    "\n",
    "# Add book reviews\n",
    "for index, row in df_books.iterrows():\n",
    "    data.append({'text': row['review/text'], 'sentiment': 'unknown', 'topic': 'book'})\n",
    "\n",
    "# Convert to a DataFrame\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "# If you have or need sentence IDs\n",
    "final_df = final_df.reset_index().rename(columns={'index': 'sentence id'})\n",
    "\n",
    "# Show the dataframe\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/andreeagiurgiu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeagiurgiu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence   topic sentiment\n",
      "0                                   absolute garbage  sports   neutral\n",
      "1  9 out of 10 games of pvp will matchmake you ag...  sports   neutral\n",
      "2                                      terrible game  sports  negative\n",
      "3  I personally haven't experienced any major bug...  sports   neutral\n",
      "4  Base game is pretty fun but it is even better ...  sports  positive\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sent_tokenize\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentence_data = []\n",
    "\n",
    "# Function to analyze sentiment of individual sentences\n",
    "def analyze_sentence_sentiment_and_store(text):\n",
    "    if not isinstance(text, str):\n",
    "        return\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Analyze sentiment for each sentence\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 1:\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            sentiment = 'positive' if ss['compound'] >= 0.05 else 'negative' if ss['compound'] <= -0.05 else 'neutral'\n",
    "            sentence_data.append({'sentence': sentence, 'topic': row['topic'], 'sentiment': sentiment})\n",
    "\n",
    "# Apply sentiment analysis to the sports and books reviews\n",
    "for index, row in final_df.iterrows():\n",
    "    if row['topic'] != 'movie':  # Assuming movie reviews already have sentiment\n",
    "        analyze_sentence_sentiment_and_store(row['text'])\n",
    "\n",
    "# Create a new DataFrame from the sentence-level data\n",
    "sentence_df = pd.DataFrame(sentence_data)\n",
    "\n",
    "# Display the updated DataFrame with sentence-level sentiment\n",
    "print(sentence_df.head())\n",
    "\n",
    "# Save the sentence-level data to a text file\n",
    "sentence_df.to_csv('sentence_level_output.txt', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.38      0.52    115362\n",
      "     neutral       0.88      0.26      0.40    151818\n",
      "    positive       0.63      0.98      0.76    308358\n",
      "\n",
      "    accuracy                           0.67    575538\n",
      "   macro avg       0.78      0.54      0.56    575538\n",
      "weighted avg       0.74      0.67      0.62    575538\n",
      "\n",
      "tfidf\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.42      0.56    116092\n",
      "     neutral       0.87      0.32      0.47    151434\n",
      "    positive       0.65      0.97      0.78    308012\n",
      "\n",
      "    accuracy                           0.69    575538\n",
      "   macro avg       0.78      0.57      0.60    575538\n",
      "weighted avg       0.74      0.69      0.65    575538\n",
      "\n",
      "tfidf\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.43      0.57    116078\n",
      "     neutral       0.87      0.33      0.48    151641\n",
      "    positive       0.65      0.97      0.78    307819\n",
      "\n",
      "    accuracy                           0.69    575538\n",
      "   macro avg       0.78      0.58      0.61    575538\n",
      "weighted avg       0.75      0.69      0.66    575538\n",
      "\n",
      "airline_count\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.64      0.66    115626\n",
      "     neutral       0.86      0.65      0.74    151764\n",
      "    positive       0.79      0.90      0.84    308148\n",
      "\n",
      "    accuracy                           0.78    575538\n",
      "   macro avg       0.78      0.73      0.75    575538\n",
      "weighted avg       0.79      0.78      0.78    575538\n",
      "\n",
      "airline_count\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.65      0.67    115679\n",
      "     neutral       0.85      0.68      0.76    151448\n",
      "    positive       0.80      0.89      0.84    308411\n",
      "\n",
      "    accuracy                           0.79    575538\n",
      "   macro avg       0.78      0.74      0.76    575538\n",
      "weighted avg       0.79      0.79      0.79    575538\n",
      "\n",
      "airline_count\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.66      0.67    115561\n",
      "     neutral       0.85      0.69      0.76    151705\n",
      "    positive       0.80      0.89      0.85    308272\n",
      "\n",
      "    accuracy                           0.79    575538\n",
      "   macro avg       0.78      0.75      0.76    575538\n",
      "weighted avg       0.79      0.79      0.79    575538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_data(min_val, train_setting):\n",
    "    \n",
    "    # Treat all sentences as string\n",
    "    sentence_df['sentence'] = sentence_df['sentence'].astype(str)\n",
    "    \n",
    "    vectorizer = CountVectorizer(min_df = min_val, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "    text_counts = vectorizer.fit_transform(sentence_df['sentence'])\n",
    "\n",
    "    if train_setting == \"tfidf\":\n",
    "        # Convert raw frequency counts into TF-IDF values\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        setting = tfidf_transformer.fit_transform(text_counts)\n",
    "    \n",
    "    else:\n",
    "        setting = text_counts\n",
    "\n",
    "    #use training data to build classifier\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        setting, # model\n",
    "        sentence_df['sentiment'], # the category values for each tweet \n",
    "        test_size = 0.20 # we use 80% for training and 20% for development\n",
    "        ) \n",
    "    # Train a Multimoda Naive Bayes classifier\n",
    "    clf = MultinomialNB().fit(docs_train, y_train)\n",
    "\n",
    "    # Predicting the Test set results, find macro recall\n",
    "    y_pred = clf.predict(docs_test)\n",
    "    target_names = [str(label) for label in sorted(set(y_test))]\n",
    "    \n",
    "    #classification report\n",
    "    return classification_report(y_test, y_pred, target_names = target_names)\n",
    "\n",
    "min_df = [2, 5, 10]\n",
    "dif_settings = ['tfidf', 'airline_count']\n",
    "\n",
    "#results of the experiments with the different settings\n",
    "for model in dif_settings:\n",
    "    for df in min_df:\n",
    "        print(model)\n",
    "        print(df)\n",
    "        print(train_data(df, model))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
